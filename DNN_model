import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping
from sklearn.model_selection import train_test_split
import numpy as np
import glob
import matplotlib.pyplot as plt

# 载入所有txt文件路径
file_pattern = 'path_to_your_txt_files/*.txt'  # 替换为你的txt文件路径模式
txt_files = glob.glob(file_pattern)

# 超参数
initial_learning_rate = 0.001
epochs = 100
warmup_epochs = 10
decay_rate = 0.1
batch_size = 32  # 每批次加载的样本数

# 自定义学习率调度函数
def scheduler(epoch, lr):
    if epoch < warmup_epochs:
        return initial_learning_rate * (epoch + 1) / warmup_epochs
    else:
        return initial_learning_rate * decay_rate ** ((epoch - warmup_epochs) / (epochs - warmup_epochs))

# 创建生成器函数
def data_generator(txt_files, batch_size):
    while True:
        for file in txt_files:
            data_part = np.loadtxt(file)  # 加载整个文件的数据
            np.random.shuffle(data_part)  # 随机打乱数据
            X = data_part[:, :-1]  # 假设最后一列是标签
            y = data_part[:, -1]
            num_batches = len(X) // batch_size

            for i in range(num_batches):
                start_idx = i * batch_size
                end_idx = (i + 1) * batch_size
                yield X[start_idx:end_idx], y[start_idx:end_idx]

# 划分训练集和验证集
train_files, val_files = train_test_split(txt_files, test_size=0.2, random_state=42)

# 创建DNN模型
input_dim = 200
model = Sequential([
    Dense(256, activation='relu', input_shape=(input_dim,)),
    BatchNormalization(),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dense(16, activation='relu'),
    BatchNormalization(),
    Dense(8, activation='relu'),
    BatchNormalization(),
    Dense(1, activation='sigmoid')  # 二元分类，使用sigmoid作为输出激活函数
])

# 编译模型
optimizer = Adam()
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# 学习率调度回调
lr_scheduler = LearningRateScheduler(scheduler)

# 早停策略
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# 计算总样本数
num_train_samples = sum(len(np.loadtxt(file)) for file in train_files)
steps_per_epoch = num_train_samples // batch_size

# 训练模型（使用生成器）
history = model.fit(data_generator(train_files, batch_size), epochs=epochs, 
                    steps_per_epoch=steps_per_epoch, callbacks=[lr_scheduler, early_stopping],
                    validation_data=data_generator(val_files, batch_size), 
                    validation_steps=len(val_files) // batch_size)

# 提取训练过程中的损失和准确率
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epochs = range(1, len(train_loss) + 1)

# 绘制损失曲线
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# 绘制准确率曲线
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# 保存模型
model.save('your_model.h5')
